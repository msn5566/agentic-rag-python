# Project Documentation: Gemini-Powered RAG System

---

## 1. Introduction

This document provides an overview of the Gemini-Powered Retrieval-Augmented Generation (RAG) API, a system designed to address the challenge of information retrieval from a large and growing library of unstructured documents (`.pdf`, `.docx`, etc.).

The core problem this system solves is the inefficiency of finding specific, context-aware answers within documents. Traditional keyword searches often fail to capture nuance and provide direct answers. This project enables users to ask natural language questions and receive accurate, concise answers generated by an AI that uses the documents as its knowledge base.

The solution is a RAG system that combines two powerful AI techniques:
1.  **Retrieval:** A vector database (ChromaDB) finds the most relevant text snippets from the document library based on a user's query.
2.  **Generation:** A Large Language Model (Google's Gemini 1.5 Flash) uses these retrieved snippets as context to formulate a human-like answer.

---

## 2. Key Features

-   **Intelligent Q&A:** Uses Google's `gemini-1.5-flash-latest` to provide context-aware answers.
-   **High-Quality Embeddings:** Leverages `text-embedding-004` for accurate document retrieval.
-   **Automatic Indexing:** A background scheduler automatically detects and processes new or modified documents in the `uploads` folder.
-   **RESTful API:** Simple endpoints for uploading, querying, and deleting documents.
-   **Persistent Vector Store:** Uses ChromaDB to store document embeddings, ensuring data is not lost on restart.
-   **PII Masking:** Automatically detects and masks Personally Identifiable Information (PII) in generated answers for enhanced privacy.
-   **Streaming Responses:** Queries return answers as they are generated for a responsive user experience.

---

## 3. System Architecture

The system is composed of several key components that work together:

-   **FastAPI Server (`app_google.py`):** Provides the web interface for interacting with the RAG system. It handles file uploads, queries, and document deletion.
-   **RAG Core Engine (`rag_google.py`):** Contains the fundamental logic for the RAG pipeline, including document loading, chunking, retrieval, and answer generation using LangChain.
-   **Google Generative AI:** Powers both the document embedding (vector creation) and the final answer generation (LLM).
-   **ChromaDB:** A persistent vector database used to store and efficiently search through the document embeddings.
-   **Background Scheduler (`apscheduler`):** A background process that periodically scans the `uploads` directory to keep the knowledge base synchronized.

```
                               +----------------------+
                               | Google Gemini LLM    |
                               +----------+-----------+
                                          ^
                                          | 5. Generate Answer
                                          |
+------+   1. API Request   +-----------------+   4. Send Query + Context   +----------------+
| User |------------------->|  FastAPI App    |---------------------------->| RAG Core Engine|
+------+  (Upload, Query)  | (app_google.py) |                             | (rag_google.py)|
         <------------------|                 |<----------------------------|                |
          6. Stream Answer |                 |   3. Retrieve Context       +-------+--------+
                           +--------+--------+                                     |
                                    | 2. Index / Store                            |
                                    v                                             v
                           +--------+--------+                           +--------+--------+
                           |  Uploads Folder |                           | Chroma VectorDB |
                           +-----------------+                           +-----------------+
                                    ^
                                    |
                           +--------+--------+
                           |  Auto-Scheduler |
                           +-----------------+
```
---

## 4. Core Logic and Workflow

### 4.1. Ingestion and Indexing

The process begins when documents are added to the `uploads` folder, either via the `/upload` API endpoint or by a user placing files there directly.

-   **Load & Chunk (`load_docs`, `chunk_docs`):** The system accepts `.pdf` and `.docx` files and splits them into smaller, overlapping text "chunks" (approx. 1200 characters). This is crucial for finding precise context later.
-   **Vectorize & Store (`add_to_vectorstore`):** Each text chunk is converted into a numerical vector (an "embedding") using Google's `text-embedding-004` model. These embeddings are then stored in the persistent ChromaDB vector store.

---

### 4.2. Automated Indexing

The system is designed to be low-maintenance and always up-to-date.

-   **Startup Scan:** On server start, the system scans the `uploads/` folder for any new or modified files and indexes them.
-   **Background Scheduler:** A job runs every 5 minutes to re-scan the folder. If a new file is found, it's indexed. If a file has been modified, its old version is removed from the database and the new version is indexed. This ensures the knowledge base is always fresh.

---

### 4.3. Query, Retrieval, and Generation

When a user sends a question to the `/query` endpoint:

-   **Retrieve (`retrieve`):** The user's query is converted into a vector. ChromaDB then performs a similarity search to find the text chunks with the most similar vectors, filtering out any results below a relevance threshold to ensure quality.
-   **Generate (`answer_query_with_context`):** The retrieved text chunks are inserted into a prompt for the Gemini LLM. The model is instructed to answer the user's question based *only* on this provided context.

### 4.4. Security: PII Masking
A critical security feature is built into the generation step. The prompt instructs the LLM to detect and mask any Personally Identifiable Information (PII) before generating the final answer.
-   **Name:** `John Smith` -> `J*** S****`
-   **Email:** `test@example.com` -> `t***@example.com`
-   **Phone:** `123-456-7890` -> `***-***-**90`

---

## 5. API Endpoints

-   `POST /upload`: Accepts one or more files, saves them to the `uploads/` directory, and triggers the indexing process.
-   `POST /query`: Accepts a JSON body `{"q": "your query"}` and streams the LLM's response back in real-time.
-   `DELETE /documents`: Accepts a list of file paths to remove the corresponding documents and their chunks from the vector store.

---

## 6. Configuration and State

-   **`config_google.json`:** A central file for managing all important settings, including the Google API Key, model names, chunking parameters, and storage paths.
-   **`storage/index_state_google.json`:** This file acts as the "memory" for the auto-indexing feature. It tracks every indexed file and its last modification time, allowing the scheduler to detect new or updated files.

---

## 7. Future Enhancements

-   Building a simple web-based UI for uploads and queries.
-   Adding support for more file types (e.g., `.txt`, `.pptx`).
-   Implementing user authentication and authorization.
-   Experimenting with more advanced retrieval strategies (e.g., HyDE, re-ranking).